"""
Some standard ranking metrics
"""
import numpy as np
import operator
from functools import reduce

def dcg_at_k(r, k, method=0):
    """
    compute dcg using k relevance values
    @param r:
    @param k:
    @param method:
    @return:
    """
    """Score is discounted cumulative gain (dcg)
    Relevance is positive real values.  Can use binary
    as the previous methods.
    Example from
    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf
    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]
    >>> dcg_at_k(r, 1)
    3.0
    >>> dcg_at_k(r, 1, method=1)
    3.0
    >>> dcg_at_k(r, 2)
    5.0
    >>> dcg_at_k(r, 2, method=1)
    4.2618595071429155
    >>> dcg_at_k(r, 10)
    9.6051177391888114
    >>> dcg_at_k(r, 11)
    9.6051177391888114
    Args:
        r: Relevance scores (list or numpy) in rank order
            (first element is the first item)
        k: Number of results to consider
        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]
                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]
    Returns:
        Discounted cumulative gain
    """
    r = np.asfarray(r)[:k]
    if r.size:
        if method == 0:
            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))
        elif method == 1:
            return np.sum(r / np.log2(np.arange(2, r.size + 2)))
        else:
            raise ValueError('method must be 0 or 1.')
    return 0.


def ndcg_at_k(r, k, method=0):
    """
    Compute NDCG at k
    @param r: ranked relevance scores
    @param k: number of relevance scores to consider
    @param method:
    @return:
       ndcg score

    Score is normalized discounted cumulative gain (ndcg)
    Relevance is positive real values.  Can use binary
    as the previous methods.
    Example from
    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf
    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]
    >>> ndcg_at_k(r, 1)
    1.0
    >>> r = [2, 1, 2, 0]
    >>> ndcg_at_k(r, 4)
    0.9203032077642922
    >>> ndcg_at_k(r, 4, method=1)
    0.96519546960144276
    >>> ndcg_at_k([0], 1)
    0.0
    >>> ndcg_at_k([1], 2)
    1.0
    Args:
        r: Relevance scores (list or numpy) in rank order
            (first element is the first item)
        k: Number of results to consider
        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]
                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]
    Returns:
        Normalized discounted cumulative gain
    """
    dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)
    if not dcg_max:
        return 0.
    return dcg_at_k(r, k, method) / dcg_max


def _ndcg_for_query(queryid, query_model_score, query_judge_score, ndcg_compute_fcn, query_to_score_dict, k):
    if queryid:
        query_model_score, query_judge_score = zip(
            *sorted(zip(query_model_score, query_judge_score), key=operator.itemgetter(0)))
        query_to_score_dict[queryid] = ndcg_compute_fcn(query_judge_score, k)

_NDCG_RELEVANCE_SCORES = [31/31,15/31,7/31,3/31,0]

def _ndcg_compute_fcn(judge_scores, k):
    return ndcg_at_k([_NDCG_RELEVANCE_SCORES[i] for i in judge_scores], k, 0)

def ndcg_from_scores(queryids, model_scores, judge_scores, k, ndcg_compute_fcn=_ndcg_compute_fcn, queryids_sorted=True):
    """

    @param queryids: query ids
    @param model_scores: scores generated by the model
    @param judge_scores: relevance scores as generated by judges
    @param k: how many items to consider
    @param ndcg_compute_fcn: the function to compute NDCG from a list of judge scores and k.  this function can be
       ndcg_at_k or a lambda that uses ndcg_at_k.  see _ndcg_compute_fcn for an example
    @param queryids_sorted: true if queryids are sorted so that consecutive ids are equal
    @return:
      [average_ndcg, query_to_score_dict]
    """
    if not queryids_sorted:
        if isinstance(queryids, np.ndarray):
            i = queryids.argsort()
            queryids = queryids[i]
            model_scores = model_scores[i]
            judge_scores = judge_scores[i]
        else:
            queryids, model_scores, judge_scores = zip(*queryids_sorted(zip(queryids, model_scores, judge_scores), key=operator.itemgetter(0)))

    last_queryid = None
    query_model_score = []
    query_judge_score = []
    query_to_score_dict = {}
    for i, queryid in enumerate(queryids):
        if last_queryid==queryid:
            query_model_score.append(model_scores[i])
            query_judge_score.append(judge_scores[i])
        else:
            _ndcg_for_query(last_queryid, query_model_score, query_judge_score, ndcg_compute_fcn, query_to_score_dict, k)
            last_queryid = queryid
            query_model_score = [model_scores[i]]
            query_judge_score = [judge_scores[i]]

    _ndcg_for_query(last_queryid, query_model_score, query_judge_score, ndcg_compute_fcn, query_to_score_dict, k)
    return np.mean(query_to_score_dict.values()), query_to_score_dict
